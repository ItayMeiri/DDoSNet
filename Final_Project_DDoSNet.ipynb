{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a479a631",
   "metadata": {},
   "source": [
    "# DDoSNet2019\n",
    "\n",
    "This project implements the RNN autoencoder model detailed in this paper: https://arxiv.org/pdf/2006.13981.pdf\n",
    "\n",
    "\n",
    "We used the **CIC-DDoS2019** dataset, which contains a _\"comprehensive variety of DDoS\n",
    "attacks and addresses the gaps of the existing current datasets_\". \n",
    "\n",
    "The dataset contains over 30GB of data, and over 70 million lines, spread over 18 different files detailing different attacks.\n",
    "\n",
    "Dealing with such files on a typical desktop computer requires some work arounds;\n",
    "\n",
    "- We combined the csv files into one large csv file. The data contained over 5.5 billion cells after removing multiple features. \n",
    "\n",
    "- To use less memory, we had to process the data in chunks.\n",
    "\n",
    "- We saved the model after processing each chunk, so that we can continue training it at a later time. \n",
    "\n",
    "- We used callbacks to monitor the loss, with a patience of 3 epochs.\n",
    "\n",
    "- Due to the using chunks, we had to change the logic of model in run-time.\n",
    "\n",
    "\n",
    "_note: the code contains comments with further detail about some aspects_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab443b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# generate large CSV from the files from https://www.unb.ca/cic/datasets/ddos-2019.html\n",
    "\n",
    "files = [\"Portmap.csv\", \"NetBIOS.csv\", \"LDAP.csv\", \"MSSQL.csv\", \"UDP.csv\", \"UDPLag2.csv\", \"Syn2.csv\", \"DrDoS_NTP.csv\",\n",
    "         \"DrDoS_DNS.csv\", \"DrDoS_LDAP.csv\", \"DrDoS_MSSQL.csv\", \"DrDoS_NetBIOS.csv\", \"DrDoS_SNMP.csv\", \"DrDoS_SSDP.csv\",\n",
    "         \"DrDos_UDP.csv\", \"UDPLag.csv\", \"Syn.csv\", \"TFTP.csv\"]\n",
    "\n",
    "first = True\n",
    "counter = 0\n",
    "for file in files:\n",
    "    file_path = \"data/combined/\" + file\n",
    "    for chunk in pd.read_csv(file_path, low_memory=False, chunksize=1048576):\n",
    "        chunk.dropna()\n",
    "        counter+= 1\n",
    "        print(counter)\n",
    "        # create the new file\n",
    "        if first:\n",
    "            chunk.to_csv('all_data.csv', index=False)\n",
    "            first = False\n",
    "            continue\n",
    "        chunk.to_csv('all_data.csv', mode='a', header=False, index=False)  # append the data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2b6a24",
   "metadata": {},
   "source": [
    "Throughout our development, we have noticed that the code runs better on CPU, and because of the inherent randomness of the model, we used a set random seed. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac01e92",
   "metadata": {},
   "source": [
    "We have implemented the Model API, the model below has 3 steps\n",
    "\n",
    "1) Pretraining step - the data will only go through the encoder/decoder layers. \n",
    "\n",
    "2) Training step - the data will only go through the SimpleRNN layer\n",
    "\n",
    "3) Finish step - the data will go through the autoencoder, and then the final layer before outputting a result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf23c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\" # My CPU is faster than my GPU! Can be removed for GPU tensorflow. \n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, losses\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "\n",
    "total_rows = sum(1 for row in open('all_data.csv', 'r')) # computationally efficient way to sum the lines. \n",
    "\n",
    "# seed to remove randomness and reproduce results\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7e249b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AnomalyDetector(Model):\n",
    "    def __init__(self):\n",
    "        super(AnomalyDetector, self).__init__()\n",
    "        self.pretrained = False\n",
    "        self.finished_training = False\n",
    "        self.encoder = tf.keras.Sequential([\n",
    "            layers.SimpleRNN(64, activation=\"relu\", return_sequences=True),\n",
    "            layers.SimpleRNN(32, activation=\"relu\", return_sequences=True),\n",
    "            layers.SimpleRNN(16, activation=\"relu\", return_sequences=True),\n",
    "            layers.SimpleRNN(8, activation=\"relu\", return_sequences=True)])\n",
    "\n",
    "        self.decoder = tf.keras.Sequential([\n",
    "            layers.SimpleRNN(16, activation=\"relu\", return_sequences=True),\n",
    "            layers.SimpleRNN(32, activation=\"relu\", return_sequences=True),\n",
    "            layers.SimpleRNN(64, activation=\"relu\", return_sequences=True),\n",
    "            layers.SimpleRNN(79, activation=\"sigmoid\")])\n",
    "\n",
    "        self.final = tf.keras.Sequential([\n",
    "            layers.SimpleRNN(1, activation=\"sigmoid\")\n",
    "            # The orignal model uses a two-class softmax layer.\n",
    "            # However, according to the documentation, the tensorflow sigmoid activation is equivalent to a two-class softmax.\n",
    "            # There is no reason for using a softmax layer, since we only have binary classification\n",
    "            \n",
    "            # In the future, a softmax layer can be made for fine or medium-grained classification. \n",
    "        ])\n",
    "\n",
    "    def call(self, x):\n",
    "        decoded = None\n",
    "        if self.finished_training:\n",
    "            x = tf.expand_dims(x, -1)\n",
    "            encoded = self.encoder(x)\n",
    "            decoded = self.decoder(encoded)\n",
    "            decoded = tf.expand_dims(decoded, -1) # re-expanding the dimensions.\n",
    "            final = self.final(decoded)\n",
    "            return final\n",
    "        x = tf.expand_dims(x, -1)\n",
    "        if not self.pretrained:\n",
    "            encoded = self.encoder(x)\n",
    "            decoded = self.decoder(encoded)\n",
    "        final = self.final(x)\n",
    "        if self.pretrained:\n",
    "            return final\n",
    "        return decoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959edb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = AnomalyDetector() # create a model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b3f576",
   "metadata": {},
   "source": [
    "Our model uses a learning rate of 0.00001 - which showed the best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837ce1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.00001),\n",
    "                    loss=tf.keras.losses.CategoricalCrossentropy())\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "\n",
    "# compile the model, create an early stopping callback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d70486c",
   "metadata": {},
   "source": [
    "We use 70% of the data for training, and 30% for validation. The training data is further split. \n",
    "\n",
    "As part of preprocessing the data,\n",
    "\n",
    "During the preprocessing stage, we turn all \"BENIGN\" labels to 0s and 1s otherwise, as well as splitting the labels from the rest of the data. \n",
    "\n",
    "\"na\" and infinity values are dropped as well as all socket features - this is done to prevent overfitting of the model to the socket data which can vary from networks. We're training the model based on packet features alone. \n",
    "\n",
    "Lastly, we also normalize the data in order to improve our model. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1c292d",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"all_data.csv\" # the large file created\n",
    "\n",
    "current_row = 0\n",
    "limit = int(total_rows * 0.7) # We use 70% of the data for training(which is also split into training/testing, 30% for validation\n",
    "for chunk in pd.read_csv(file_path, low_memory=False, chunksize=1000000):\n",
    "    current_row += 1000000\n",
    "    print(\"Current chunk: \", current_row)\n",
    "    pd.set_option('use_inf_as_na', True)\n",
    "    chunk = chunk.dropna()\n",
    "    chunk.head()\n",
    "    to_drop = ['Flow ID', ' Source IP', ' Source Port', ' Destination IP', ' Destination Port', ' Protocol',\n",
    "               ' Timestamp',\n",
    "               'SimillarHTTP', ' Timestamp']\n",
    "    \n",
    "    \n",
    "    # data proprocessing stage\n",
    "    chunk = chunk.drop(columns=to_drop, axis=1)\n",
    "\n",
    "    chunk[' Label'] = (chunk[' Label'] != \"BENIGN\").astype(int) # \n",
    "    raw_data = chunk.values\n",
    "\n",
    "    chunk.head()\n",
    "\n",
    "    # The last element contains the labels\n",
    "    labels = raw_data[:, -1]\n",
    "\n",
    "    # The rest of the data \n",
    "    data = raw_data[:, 0:-1]\n",
    "\n",
    "    train_data, test_data, train_labels, test_labels = train_test_split(\n",
    "        data, labels, test_size=0.25, random_state=42\n",
    "    )\n",
    "\n",
    "    min_val = tf.reduce_min(train_data)\n",
    "    max_val = tf.reduce_max(train_data)\n",
    "\n",
    "    train_data = (train_data - min_val) / (max_val - min_val)\n",
    "    test_data = (test_data - min_val) / (max_val - min_val)\n",
    "\n",
    "    train_data = tf.cast(train_data, tf.float32)\n",
    "    test_data = tf.cast(test_data, tf.float32)\n",
    "\n",
    "    train_labels = train_labels.astype(bool)\n",
    "    test_labels = test_labels.astype(bool)\n",
    "\n",
    "    normal_train_data = train_data[train_labels] # this is fed to the autoencoder\n",
    "    normal_test_data = test_data[test_labels]\n",
    "\n",
    "    anomalous_train_data = train_data[~train_labels] # not used in our case, but still something you can test on.\n",
    "    anomalous_test_data = test_data[~test_labels]\n",
    "\n",
    "    # if we have trained over 70% of the data, start the evaluation phase.\n",
    "    if current_row < limit:\n",
    "        autoencoder.pretrained = False\n",
    "        size_to_train = int(len(normal_train_data) * 0.1)\n",
    "        pretraining_normal_train_data = normal_train_data[0:size_to_train]\n",
    "        autoencoder.fit(pretraining_normal_train_data, pretraining_normal_train_data,\n",
    "                        epochs=1,\n",
    "                        batch_size=32,\n",
    "                        validation_data=(test_data, test_data), callbacks=[callback],\n",
    "                        shuffle=True)\n",
    "\n",
    "        autoencoder.pretrained = True\n",
    "\n",
    "        autoencoder.fit(normal_train_data, normal_train_data,\n",
    "                        epochs=1,\n",
    "                        batch_size=32,\n",
    "                        validation_data=(test_data, test_data),\n",
    "                        shuffle=True)\n",
    "\n",
    "        my_predictions = autoencoder.predict(data)\n",
    "        my_predictions = np.rint(my_predictions)\n",
    "\n",
    "        print(\"Partial check\")\n",
    "        print(\"Accuracy = {}\".format(accuracy_score(labels, my_predictions)))\n",
    "        print(\"Precision = {}\".format(precision_score(labels, my_predictions)))\n",
    "        print(\"Recall = {}\".format(recall_score(labels, my_predictions)))\n",
    "        print(\"Saving model. Chunk is currently \", current_row)\n",
    "        print(\"The total chunks are: \", total_rows)\n",
    "        print(\"70%: \", limit) \n",
    "        autoencoder.save(\"AnomalyDetectorModel\")\n",
    "        print(\"Saving complete.\")\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        print(\"Training complete. Saving model with finished_training = True\")\n",
    "        autoencoder.finished_training = True\n",
    "        autoencoder.save(\"AnomalyDetectorModel\")\n",
    "        print(\"Done.\")\n",
    "\n",
    "        print(\"Attempting to reconstruct model from save...\")\n",
    "        reconstructed_model = keras.models.load(\"AnomalyDetectorModel\")\n",
    "        print()\n",
    "        print(\"Testing accuracy on unseen data:\")\n",
    "        print(\"Chunk:\", current_row)\n",
    "\n",
    "        predictions = reconstructed_model.predict(data)\n",
    "        preds = np.rint(predictions)\n",
    "\n",
    "        print(\"Accuracy = {}\".format(accuracy_score(labels, preds)))\n",
    "        print(\"Precision = {}\".format(precision_score(labels, preds)))\n",
    "        print(\"Recall = {}\".format(recall_score(labels, preds)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f1e39e",
   "metadata": {},
   "source": [
    "Future work should focus on a few things;\n",
    "\n",
    "- Testing the model on different datasets.\n",
    "\n",
    "- Adding finer classification - the data contains labels about the type of attacks. \n",
    "\n",
    "Results:\n",
    "\n",
    "Accuracy = __0.9949557173639242__\n",
    "\n",
    "Precision = __0.9949557173639242__\n",
    "\n",
    "Recall = __1.0__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d3309d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
